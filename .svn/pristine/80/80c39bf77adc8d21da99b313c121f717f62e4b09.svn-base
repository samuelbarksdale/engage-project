Cover Page

 

 

# 1. INTRODUCTION



## 1.1 Background

#### 1.2 Document Overview

​	The goal of this document is to primarily communicate product specifications between internal developers, our customer Larry Herman, and our professor and project sponsor Dr. Jim Purtilo. Our product is designed to solve the problems stated by Larry Herman with extra features specified by Dr. Purtilo's desires but contingent on our scope. The ultimate outcome is a balance between both stakeholders.



#### 1.3 Purpose



####1.4 Goals

​	The goal of this project is two-fold. The project will develop a FERPA-compliant, stand-alone system which will decrease educators' level of effort for menial managerial tasks and decreases the inherent informational asymmetric involved between students and educators within large class spaces. Our system will automate or at least digitalize labor-intensive processes like absence reporting and attendance collection, as well as to provide analytics about students to professors so that they can take action during the course to improve it. 

#### 1.5 Audience



#### 1.6 Scope



## 1.7 Definitions

**Project Definitions**

1. *Users* refer to any student who is participating in the engagement system
2. *Admins* refer to any educator who has authority over lectures and classes
3. *Disruptive* refers to any process which fundamentally breaks from an educator's normal teaching period or which unexpectedly halts an educator's teaching period. 
4. *Engagement* refers to the general set of metrics which describe how well a student is participating and paying attention in class. These metrics include: attendance, attention, sentiment, comprehension. 
5. *Attention* refers to the amount of time a student spends listening to the educator. A student will low attention might be on their phone, or might consistently leave lecture early.
6. *Sentiment* refers to the general feeling a student has about a lecture. This is likely a spectrum or multiple spectrums of feelings.
7. *Comprehension* refers to the amount and depth of information a student retains from lecture. It's synonymous with understanding.

**Technical Definitions**

1. *API*: Application Programming Interface

2. *Microservice*: 

    

    

# 2.  Product Description

​	In the following sections we detail the product we have designed to solve the problems expressed by the stakeholders.

## 2.1     Problem Statement

​	Educators struggle to track non-grade metrics like attendance and attention within large lectures and classes. They want to improve the learning outcomes of their students. Miscommunication during lectures can drive students to lower their attendance or overall course engagement. These "at-risk" students are mostly invisible to educators who can't possible remember all three hundred faces and their respective attendances. By digital login and tracking attendance, we can enable educators to discover problematic trends in their students and act upon them. 



## 2.2 Product Overview

​	Our product is segmented across at least three main components: the attendance service, the absence service, and the Engage management UI. The entire solution will be implemented through a microservice architecture.  Each component is contained within a module and operates independently from each other, reducing technical dependencies, increasing reusability, and allowing for easy extensibility by us or future developers. 

​	Yet, the deliverable application will combine each component through codified HTTP API standards to create a seamless workflow for both educators and students. Both types of users will be able to create, read, update, and delete data, dependent on their permissions, through the front-end application. This front-end application updates and is updated by the microservice back-end. 



###2.3 Customer Characteristics

** **

​	There are two main end-users for our system: educators and  students.

##### Educators

​	 Educators will interface with the system to perform the majority of the key functionalities presented. First and foremost, the application provided will allow a place for educators(s) to see aggregated attendance information for individual students. This information will be collected whenever the educators(s) deem fit during the lecture. The application will also show basic descriptive statistics on class-wide attendance information. The application will also provide means of aggregating information offered through the class engagement activities. This information will be aggregated and summarized in brief snapshots for the educators(s) to display to their class should they want to. Additionally, educators(s) will have the ability to review and assess the numerous incoming absence request forms and approve/deny them based on the information provided. This approval process will also provide educators(s) with a means of reaching out to the student(s) that will be absent and notify them of missed activities or possible deadline extensions for assignments. Finally, the application will provide means for the educators(s) to view aggregated attendance data, along with other desired metrics such as submit server information, in an aggregated fashion where they can use different sources of information to cross-correlate and get a more accurate understanding of how not only the entire class is faring, but how individual students are doing as well. 

##### Students

​	Students will be mainly interfacing with our application for four key functionalities. The first is their ability to check-in to class when specified by the professor through the geolocation capabilities of our application. This information will be automatically recorded and will also be presented to the student - although they will only be able to access their own attendance records - this in turn would fulfill the second key requirement. The third key functionality is the ability for students to submit absence request forms to their instructor(s) in the case that they will not be able to make it to class, and be able to receive any feedback from the instructor based on the date(s) of absence. Finally, students should have the ability to to interact with in-class engagement activities set forth by the professor during lectures in the form of live questionnaire responses. In terms of specific platforms, we are aiming to get a mobile-friendly web product available to the students, however should time permit, we will also tbe aiming towards native mobile applications to help ease the overall workflows. If we have more time left-over even after that, we would be looking into developing Elms plugins to help limit the number of extra software's required for the students and the instructor(s).

  

### 2.4 User Storylines

​	Here's Ed. **Ed is an Educator**. He's having some issues teaching students. He's too busy. There are too many menial administration tasks that need to be done, like marking attendance. What's more, his lectures are so large, he doesn't know who's who and what they're all thinking. Ed just wants to do what he loves, teach!

​	One of Ed's colleagues, Larry Herman, told him of a neat educational management system, called Engage. Ed is astonished, this is just what he needs. He navigates to the Engage website or downloads the mobile application, then **creates an Educator account**. 

​	The system prompts Ed to **create a class**. Ed decided to manage his classes next semester with Engage, so he create two classes. For each class, he **inputs start and end times, building locations, sections, capacity, and student rosters**. 

​	When Ed finishes setting up his class space, he sees them arranged on a dashboard. Clicking on one brings up a more detailed view of the class. Since the semester hasn't started yet, he doesn't see much activity, just some empty graphs.

​	The summer is at a close and the semester starts back up. Ed is ready and excited to test out Engage's educational management features. On the first day of class, Ed wants students to join his class, so he tells his **students to download the Engage mobile application**. Every registering student needs a class code, so next, he projects the **class code found in his administrator's class view**.

​	Here's Stu. **Stu is a Student**. She has a class with Educator Ed. Per his instructions, **she downloads the mobile application** on her Apple iPhone. She launches the application. A sign-in page connected to the University's CAS pops up. After signing in, the application prompts her to enter a class code. She enters the code Ed projected. Success! She's now registered for Ed's class. There is another bright, happy message telling her attendance has been checked. Good job, Stu! The next time she attends lecture, she won't have to even pull out her phone. **Engage will automatically detect when she arrives in the lecture hall and will report her attendance accordingly.** 

​	After data points start coming in, Ed can begin **viewing metrics graphically**. There are graphs **showing daily, weekly, and monthly attendance**. Ed can even **display metrics from just one class or just one student** if he wants. He can also **view attention metrics**, gauging for how long students are browsing their phones, or how many of them leave lecture early. 

​	Ed receives an email from Stu, the student. Stu is informing Ed that she has been absent due to an illness. Ed groans, managing absences are always painful. Thankfully, Engage has got him covered. **Ed remembers Engage has a built-in absence reporting system**. He forwards instructions to Stu.

​	Stu wants to mark her absent days and submit documentation so she can get her absences excused. She opens the mobile application and **selects the absence reporting tab**. **She notes the days she was gone, provides a rationale, and uploads the note her doctor gave her**. Now, she must wait for the educator.

​	Ed receives Stu's report, and **approves it**. **Engage will automatically update Stu's attendance record to adjust for her excused absences.** Then, if Ed uploaded a course schedule, Engage will inform both Ed and Stu of the material that Stu missed. That way, Stu can get caught up, and Ed doesn't have to cross-examine his schedule with her absences. 

​	Unfortunately, Stu begins to struggle in Ed's class. She starts browsing Instagram during lecture, leaves early, and sometimes never shows up. Luckily, Engage is looking out for her. The next time Ed opens the dashboard, the **app notifies him about Stu's difficulty engaging with the course**. From there, Ed can reach out and attempt to address some of her issues before it's too late. 

​	At the end of the semester, Ed is quite happy. He can view attendance and attention metrics and get a better sense of the coming and goings of his course. He also didn't have to spend time tracking down absence reports, it's all automatic. He even sent out **personalized after-hours notifications to at-risk students**. Did his personalized attention work? He can check attendance metrics of those students and find out. Ed is glad he chose Engage, but he hasn't even touched the **polling or survey features, which provide insight in student sentiment and comprehension**. No worries, just like every educator's teaching style, Engage is flexible. But, when Ed is ready to increase his *"LectureIQ"*, Engage will be waiting.



### 2.5 Product Components

#### 2.5.1 Attendance Service

###### 

​	The attendance microservice will handle the storage and management of attendance data. It will do so through an exposed API with access-controlled CRUD routes. We decided to construct the attendance service as a decoupled API because it allows for an ambiguous client to post and get attendance data without being bogged down in possibly varying system details. In other words, the service is system-agnostic. 

​	When the properly authorized attendance collection client makes a request to post log a student's attendance, the service will check the client's authorization and upon success, process the data as necessary, and save it to a persistent database. 

​	

In particular the attendance service shall provide the following functionality:

1. Service shall authenticate and authorize requests to enable permissions on data access.
2. Service shall expose an api route to update a student's attendance for a particular class.
3. Service shall expose an api route to read attendance data for a particular class or student.
4. Service shall expose query functionality to filter and limit attendance data based upon semesters.
5. Service shall expose an api route for administrators to update a student's attendance.
6. Service shall expose an api route to return a binary answer for a student's attendance of a requested class.
7. Service shall expose an api route to return time data for when a student was in attendance.
8. Service shall expose an api route for the verification of student's location during attendance collection.
9. Service shall allow for cancelling a class attendance.
10. Service shall **not** store student location data, if student location data becomes relevant to the solution.



#### 2.5.2 Absence Service

​	The absence microservice will handle the reporting of student absences. Like the attendance service, the absence service will be an API and database with proper access-controlled CRUD routes. The primary data stored and handled by this service is the "report". Although we can image a more powerful and generalized report and reporting system, the proposed iteration will restrict the report to a set of user-provided inputs. When a student submits a report, they must provide the range of dates that they were absent for, the rationale for absence, the type of excused absence, and an optional file upload. How these function will be implemented in the UI is left up to the development. 

​	

1. Service shall expose an api route to create a report for a specified course and student.
2. A report shall consist of a required free text rationale, a required selection of dates missed, a required selection of for type of absence, and an optional file upload for supporting documentation.
3. Service shall expose an api route to read all absence requests for a particular course or student. 
4. Service shall expose an api route to update the current state of a request. 
5. Service shall contain four states: "Waiting for Approval", "Waiting for Change", "Approved", and "Denied".
6. Service shall expose an api route to read a student's absence history.
7. Service shall expose an api route to read absence requests with filtering by status or course.
8. Service shall expose an api route to create course schedules.
9. Service shall expose an api route update course schedules.
10. Service shall expose an api route to read a student's missing assignment list.



####2.5.3  Front-end Application

​	The normal use cases of students and educators will be handled visually through a web and mobile based front-end application. This application will provide a convenient and familiar mechanism for students and educators to interact with the management back-end. The complete functionality of the front-end applications will change between user types but will remain fairly constant between the Web, Android, and iOS implementations. To ensure the relative ease of cross-platform development, we will utilize the React Native framework, which allows for a React-enabled web application to be run on mobile devices, natively, through specific bindings. 

​	**Note**: Although our product will revolve around the front-end application as a client for the back-end, we chose the microservice framework so that other authorized developers can create additional UIs with separate scopes or programmatically scrape data off the microservice mesh for usage in custom data analytics. 

##### Implementation Rationale

​	There was considerable technical debate around the specific implementation of the front-end application. Ultimately, we decided upon a hybrid decision of web and mobile. Despite the additional complexity of mobile development, both platforms provide unique advantages separately, and a robust system when used in conjunction.

​	From a human-computer interaction perspective, we find it more likely that an educator would use the web platform because it suites the "power-user" administrator functions better than a mobile application. On the other hand, the simpler actions performed by the students match the form factor of a mobile application.

​	Technical considerations also influenced this decision. One of the crucial goals of this product is to provide an automated or digital means for attendance collection. Yet, UMD policy states that attendance cannot be graded for lectures, which makes incentivizing user-initiated attendance check-ins challenging. Although the educator could make a persuading argument about the benefits of cooperating in such a system, leaving such core functionality up to trust and hope allows for too great an error. 

​	There are many scenarios where a student could fail to check-in. One reason could be tardiness. A student who arrives late to class might be too flustered and could easily forget to check-in. A student could have also left momentarily while the educator was reminding the class to check-in (such a constant reminder would become tedious for both educator and student ). An educator themselves may have forgotten to remind students. Then, there is the unfortunate instance of apathetic students. It's not hard to imagine, particularly, in lower-level classes, a signficiant student body who wants to coast through college and cares little about educational outcomes and student-educator relationships. They most certainly will not undergo the hassle of manually checking-in.

​	With a mobile implementation, we can passively check a student's attendance without them performing any action. Not only is this more convenient for the student, but also captures attendance with better degree of accuracy. We can attribute an exact number of students who were verifiably in lecture. The manual check-in system is more sensitive to human and environmental factors, leaving it susceptible to false-negatives.  Additionally, a native mobile implementation allows for a greater range of future functionality and control, including but not limited to: access to device lifecycle, device status bar, device UI, file system, efficient push notifications, and native camera support. These are features we think would be beneficial to students and would outweigh the possible negatives of having a small installed application and discrete location polling. 

​	However, the web implementation offers a graceful alternative for students who are concerned about the mobile application's privacy permissions, specifically its geolocation capabilities. The web implementation will ultimately provide similar functionality for the student, it will just lack some of the convenience features like automatic check-in, native camera upload support, and 24/7 push notification support. 

​	In summary, the mobile implementation provides a set of features which garner a higher confidence in the accuracy of our data, and provide us a platform for continuous analysis and richer features. Yet, the web implementation gracefully supports privacy-minded students and offers a traditional mechanism for "power-user" administrator functions. 



##### Functionality

	1. Educator can view their active courses.
 	2. Educator can select an active course for a more detailed look.
 	3. Educator can view and switch between types of class.
 	4. Educator can view all sections of a course.
 	5. Educator can view attendance at the course level.
 	6. Educator can view attendance at the section level.
 	7. Educator can view a list of active students at the course and section level.
 	8. Educator can select a student for a more detailed look.
 	9. Educator can view a student's attendance for a particular course (by implication section, too).
 	10. Educator can view all outstanding Absence requests. 
 	11. Educator can accept, deny, or request changes for any absence requests
 	12. Educators can view an absence request in detail with all form fields visible



1. Students can view their active courses.
2. Students can view their attendance for their active courses.
3. Students can submit an absence request including a type of excuse, rationale for excuse, a selection of dates, and supporting documentation (optional).
4. Students can view if their attendance was successfully checked and manually check-in if need be.

​	



### 2.3   Lo-fi Renderings (or code)

```
Project description says teams that fail do not have lo-fi renderings or basic code examples.
```



### 2.4  Delivery Manifest

```
This is basically a checklist to summarize clearly what we're getting and how we will know we got it. It is used at delivery to, well, check that we got what you promised
```









## 2.3     Team Management

```
How we will organize to reach the timeline specified below. Which teams will we have? What are they responsible for? How will we work in parallel?
```
We will employ an Agile methodology with weekly sprints.


## 2.4     Timeline

```
What is our timeline to finish the project in the scope of the semester? What deliverables will we finish when? How will we handle taking on new features (in addition to the attendance task?)

From Purtillo: We will want you to call out at least the major tasks, story lines or activities necessary to complete your project, and show how composing them should lead to success. Then, as you proceed with the build, we will want you to maintain (e.g. on one of your groups' web sites) a graphic summary of the status of completion, so we can track progress. Are you getting closer or further away from success? The 'manager' will want to know at a glance. (We will talk more in class about a variety of tools for this, such as JIRA or Trello. We hope you will come to see such tools as aids to success, not busywork or barriers. Learn to use these to improve your productivity.) The cost estimate should be consistent with the timeline; potentially you will reflect on how to create a Gantt chart
```



## 2.5     Alternate Solutions

```
There are two great ways to ensure your green light process is painful, drawn out, complex and jam packed with frustration. One is to submit a proposal that does not include analysis of alternate design approaches with an explanation as to why one has been chosen in particular.
```
Several considerations helped hone our current design to fit the goals and scope of this project. Time is the largest constraint. While this proposal serves as a foundation for a data mesh service, creating an entire framework would require more than a semester. At an earlier design iteration, we planned an entire scaleable framework application which would be ready for any data mesh application. A rough cost analysis revealed the development of such a robust system would take more than double the time we have in a semester. Thus, we decided to focus on one microservice of the system: attendance management.
Another consideration was balancing Dr. Purtillo's expectations with that of the clients. Dr. Purtillo wanted the focus to be student engagement, while Larry Herman was more interested in an attendance management system. When designing for engagement, we considered gamification of engagement data such as questions asked and time off screen. Users could accrue virtual points, similar to Reddit "karma", and see their progress on leaderboards or in other rewards. We looked at similar apps, such as Pocket Points and Top Hat, to get ideas for possible incentives. This is discussed further in section USER ADOPTION (LINK). Ultimately, we decided to prioritize Larry Herman's desire for attendance management. Class attendance is a form of engagement; creating a foundation for simple student-instructor interactions is an essential step towards improving overall engagement.
When considering the technical specifications of the design, we considered a classroom Wifi network to which students could connect, thus verifying their attendance. This idea was not feasible for the scope of this class. Additionally, there are more refined solutions which require no extra hardware. We also considered a web-only application, however we deemed this unacceptable because a mobile app would have easier access to necessary hardware such as GPS and a camera.


#### Foreseen Risks 

##### 1. Geolocation

​	There are many technical risks surrounding the geolocation capabilities of our solution. Firstly, acutely accurate geolocation relies on a combination of GPS, cell tower triangulation, and Wi-Fi network connectivity. Tracking attendance based upon geolocation has many degrees of failure. If GPS is unable, then any form of geolocation is essentially impossible. Further, even with a GPS connection, location accuracy may be too imprecise. Relying on cell tower and Wi-Fi triangulation can help us pinpoint locations, but if anyone of those are inaccessible, then the problem remains.

​	Ostensibly, we probably need to know the exact classroom location of a student. However, there may be unique correlations in reality that we can leverage. Firstly, we can assume that interesting geolocation points are areas which contain a large amount (80+) of users during a particular time. Suppose we have many students registered in the same class. `Students 1...N` have an accurate GPS signal but `Student N+1` does not. Given these circumstances, it might be possible to claim, with a certain probability, that `Student N+1` is present in the class if they are within a certain distance threshold to `Students 1...N` .  

​	Alternatively, we might be able to leverage a slightly simpler heuristic. It may not matter what classroom the user is in after all. Given a student in a class which has a class start time and a building location, it might be accurate enough to claim: if a student is in the same building as their class during the class' interval, then it is reasonable to assume the student is in attendance. 

​	The likely solution is not clear. We will have to experiment with modern geolocation capabilities and the environmental factors on campus. 

##### 2. User Adoption

​	We have struggled to imagine why a user might want to participate in our system. Why should they download an app or log into a website? Particularly, if that application will "hold them accountable" by collecting attendance and engagement data. 

​	If we had a subtle manner of tracking attendance which doesn't depend on any user interaction, then the question would be trivial. We can imagine systems which use local cell towers to find the location of cell phones, or wireless access points which track cell phones as they automatically ping near-by Wi-Fi networks. Yet, many of those solutions seem technically advanced, costly, possibly illegal, and probably immoral.

​	Such lightweight and "hidden" solutions also lack extensibility. Since the problem can be applied to the question of engagement, generally, we desire a system capable of tracking other engagement metrics. Calculating the ratio between a student's screen time and listening time would mostly likely be intractable using clever methods like packet listening on a "lecture Wi-Fi network".

​	Looking for inspiration at educator's traditional methods, we find that lecture quizzes or polls using clicker technology are useful to gauge certain engagement metrics and as a byproduct, collect attendance. There are a few issues with this method, however. Educators must create such quizzes and must ensure they are "useful". Additionally, because digital quizzes rely on computers, educators and students run into technical issues. There is also the human factor involved. Class quizzes at the beginning of the lecture have the high potential of missing a large segment of latecomers.  Also, simply tracking attendance becomes a process. An educator must stop class for everyone to submit input. Clickers and other polling solutions also lack flexibility. Answers are multiple choice, and either you force students to use pricy calculator-like devices to ensure attendance, or risk the software outcome of students answering anywhere. Clickers are also time based.

​	Although using direct interaction with students can collect attendance, it's suboptimal. Nevertheless, digital polls are useful. They give students a reason to use our application. An educator who uses digital polling will compel students to download our application so they can complete these polls. Once the application is downloaded, we can track attendance without any input form the user. Even if the educator rarely uses polls, the possibility of them (along with additional benefits) should be an effective incentive.

​	We further imagine additional benefits which would incentivize students. Although attendance and general engagement can't really be graded, there might be room for an educator to give out "good faith" points. Students whose profile leans towards higher engagement, might be at a better bargaining position for a slight grade bump-up at the end of the semester, an assignment extension, or letter of recommendation. Further, the analytics the educator collects can directly improve the lecture, leaving the students more satisfied. Future features might even "gamify" the engagement system. Features such as a leaderboard, badges, and integration with software such as Portfolium could provide a game-like drive within the college atmosphere as well as post-college benefits. It seems likely that college engagement and participation metrics would be of great use to employers struggle to gauge a new graduate's personality, work ethic, and teamwork skills. 

3. #####Privacy



# 3. System Design

### 3.1 Core Technologies

This section describes the core technologies that will be used for each level of the development stack, as well as for other necessary functionalities such as geolocation tracking and image uploading. 

**Database:**

**Back End:** Language, framework, ORM?, 

**Front End:** reactnative, javascript

**Geolocation:**





### 3.2 Data Schemas

##### 3.2.1 Access Control

​	

| Permissions               | Educator | Student | Notes                                    |
| ------------------------- | -------- | ------- | ---------------------------------------- |
| List self data            | Yes      | Yes     |                                          |
| List student by ID        | Yes      | No      | Iff student enrolled in Educator's class |
| List students by course   | Yes      | No      | Iff course owned by Educator             |
| List student's attendance | Yes      | No      | Iff student enrolled in Educator's class |
| List self attendance      | -        | Yes     |                                          |
| Create self attendance    | -        | Yes     | Iff student meets attendance constraints |
| Create student attendance | Yes      | No      | Iff student enrolled in Educator's class |
| Update student attendance | Yes      | No      |                                          |
|                           |          |         |                                          |



### 3.3 Data Dependencies

##### 3.3.1 Authentication and Authorization

​	This document has previously referred to login and access-control systems. Rather than create a from-scratch authentication system, we will rely on the University's Central Authentication Service (CAS) which is a protocol for Single Sign-on (SSO). The University currently achieves their SSO system through Shibboleth, a privacy-centric, open-source, education-oriented SSO solution. 

​	Shibboleth works as any other SSO. A user attempts to access a protected resource and, assuming they do not have an active session, are redirected to the service provider, which will in turn submit an authentication request to the identity provider with the user's credentials. The identity provider is responsible for the authentication process, and assuming the credentials are correct will return a success response. This response will be handled by the service provider, allowing the user to access the resource.

​	Shibboleth has an excellent explanation here: https://www.shibboleth.net/index/basic/.

​	We will use the returned authentication response and the authorization user attributes to make access-control decisions. 

##### 3.3.2 Canvas API

​	Canvas is a Learning Management System (LMS) developed by Infrastructure, which forms the backbone of the ELMS system used on campus. Part of Canvas is an API that exposes data specific to the institution.

​	We introduce a dependency on Canvas for a variety of reasons. First, it automatically keeps track of multiple vectors of information which saves us considerable time when building our backend. We don't have to create data ingests for student and course data, nor save that data in a database. Secondly, it provides built-in permissions which again reduce the complexity we must introduce to our backend. Although we will implement a permission system to control CRUD access, we will not have to handle as objects like courses.

​	

# 4. TESTING

 Weekly the team will meet to set priorities over the product's development and delegate these tasks to build. Throughout this process, the product will undergo three different types of tests to verify and ultimately validate the competence of the product.

The first type of test is basic functionality, where each function of a microservice will be made sure to work correctly. For example, the excused absence service if the files submitted as evidence of valid excuse get uploaded and appropriately stored the system database.

The second type of test is Alpha testing of individual microservice, to verify that each microservice accomplishes its task with the system as a whole. Here the microservice must have all of its essential functions working to secure it's working correctly.

The last type of tests is Beta testing. With our client's aid and permission, a class of instructor Herman will be subject to test our product. Hermann will test educator account experience, where he will create a class with the services that have produced. And his students will download the app and be the subjects of the data collection and administrative functionality of the course. Here, it is crucial that the Beta testing follows without disrupting the class progress; if it does, the test will be considered a failure. If the product doesn't interrupt the workflow of the class and performs accordingly, the product will be regarded as validated.



## 4.1  User Acceptance Testing  (Beta Testing)

 **Acceptance Testing**

Acceptance testing will be carried out through verification and validation. Since we have chosen to adopt the agile development methodology, one of it’s core features is the ability for it to build off of continual feedback from the users. The agile methodology is centered around consistent user-testing that will provide vital feedback on our product that we can leverage on our next sprint and showcase on our next deadline for deliverables. In this way we also avoid the situation where changing needs are not met, as we are in constant contact with professor Herman and Professor Purtilo and they are ideally never left in the dark. That being said, while agile forces us to remain in close contact with our primary clients, one of the other key end-users of our system are the students themselves. In this sense, it is also important that we get testing environments centered around the student experience and also develop in as much as an agile setting as we can for their end of the platform. 

**Verification**

Our testing will be done in two key settings. The first will be periodic testing with Professor Herman and Professor Purtilo where we will have them interact with the instructor section of the application. The primary focus of these tests will be to ensure that all independent (out-of-classroom) workflows for the instructors are fully met and any inconsistencies can be ironed out. The following lists out the key functionality we will be observing during the tests. Verification will take place in two main groups, as stated above. 

Following the technical results, we will further inquire with both professors on whether or not they were satisfactory with their workflows for completing each of the core functionalities in the form of a simplistic survey, where they will rate their satisfaction as well as ease of use for each of the main feature modules. We will also ask for any additional comments or suggestions that they might have. 

The second setting for our user tests will take place in the classroom. We have already made arrangements with Professor Herman that we will be able to come in during a few of his lectures to his students and test our platform on either a small subset, or a large subset of the students. Additionally, during the test, two members of our team will be sitting with Prof. Herman while the rest will be available to answer any questions from the students. 

Before we start the experiment, we will do a quick brief with the entire class of what the experiments main goal is. We will let them know that they will have instructions to follow during the entire process, but should the application break or fail for any of the students that they let us know immediately, as it is important to make sure that we record any and all instances of the application breaking.

Below shows the ideal workflow that we will be looking for within our tests. 

·        In the beginning of the lecture, Professor Herman is able to successfully and timely log into his platform and will proceed to ask rest of the class to access our application through the web

·        Students will take the time to login to their account through ELMS. All students will be able to log in successfully and will be able to successfully select the current class object as it will automatically populate their dashboard

·        Students will be able to successfully check-into class through the geolocation capability of our platform

·        This attendance information will be able to populate live on Prof. Herman’s end of the system and the amount of people who are successfully able to login versus those that are recorded in Prof. Herman’s system should be equal

·        Prof. Herman should be able to quickly and easily create an engagement activity and broadcast it to the class with the push of a button. Prof. Herman can launch these engagement activities at his leisure however we request that at least two of them be performed during the lecture

·        Students will answer the engagement activities through our platform during the specified time and all of their responses will be tabulated accurately and be immediately available for view on Prof. Herman’s end of the system. 

After the test, we will be sending out a short survey once again to the students that participated in our trial. The following highlight the main questions that we will be asking – most of them will be a question that allows students to rate both the ease of use and usefulness of the various features. There will additionally be some free – response questions towards the end:

**Free Response:** 

*On a scale of 1 – 10 how easy was it to use the check-in ability?* 

*How do you compare this check-in ability to the traditional clickers? (If you have had the opportunity to us them before)*

*On a scale of 1 – 10, how useful was it for you to be able to see your own attendance data and the descriptive statistics that came along with it?* 

*On a scale of 1 – 10, how easy was it for you to participate in the engagement activity set by Professor Herman?* 

*On a scale of 1 – 10, how easy was it for you to ask for absences WITHOUT our system?* 

*On a scale of 1 – 10, how easy was it for you to ask for absences WITH our system?* 

*What features do you think are unnecessary in the application?* 

*Please add any additional comments that you might have. Please note that at this time we are not taking any requests for additional features, please feel free to comment on the features that you were able to experience and suggest improvements around them.* 

 

**Validation**

Validation of results will be take place as assessment of the success rates that we got to analyze during our different testing environments. Because of the nature of what we are recording, it is very important that we do not settle for anything less than 100% accuracy when recording student attendance information as well as when documenting incoming absence requests. For this reason, it is important that we carefully document anything that goes wrong during the trials as we want to make sure any inconsistencies are not a result of the underlying platform. 

Before the check-in process, we will take a headcount of everyone that successfully was able to submit their attendance versus their recorded information on our database. We will do the same with the engagement service to make sure our application is picking up all key activity. 

Therefore the success of our application is heavily based on the success of the above workflow mentioned and ticking all of those boxes. 

Below we have outlined more of our technical criteria for success in various aspects of our application: 

**Database Connectivity**

We will need to ensure that the data we write onto the database is accurate and gets inputted in a timely manner. In terms of the instructor portal, this means that their CRUD operations that they can leverage on their dashboard are accurately represented in the database. Linking back to our tests, this would primary take place in the individual sessions with Prof. Herman and Prof.Purtilo. We will ask them to specifically test out the admin application as much as they can and we will continually verify that nothing is broken. 

**	Absence Management**

Absence management will be catered too more carefully. A big part of this system is time sensitive as requests have to be logged in and approved in the order they came in. During our class testing environment, it is unlikely we will be able to fully test this functionality, so the majority of this application will be tested through scripts that we write and manual testing. Our goal will be to overload the system with requests but make sure the requests are logged in the time that they are received and nothing breaks again. 

**	Engagement Management**

Engagement management is something we will mainly be testing in the class environment. While we are able to test this with Prof. Purtilo and Prof. Herman individual tests, a class environment would represent the scale that we are trying to accommodate. As a result we are looking for accurate recording of information and no loss of data. We want to accurately capture the data given by students during class and so it is important that any losses do not occur because of a lack of robustness in our platform. 

**Speed**

We need to ensure that the speed of the application does not contribute to any form of disruption within the class. As a result, the relative speed of the application will be important. Heavy reliance on third party API's for fetch requests in our data might increase latency and deep queries into our database might also contribute to increased lag in our application. As a result it is important that these do not become a hindrance. As a result, we are setting the following time-limits for validation that our application is non-disruptive. Check-in will happen and be successful within the first 10 minutes of class.

Furthermore, anytime one of our services fails on the instructor side, there should be no averse consequences to the workflow of the rest of the class.                                                                                                                                                                                                                 

# 5. COST ESTIMATE

```
Make sure you have a clear and supported prediction of what it will take to get you from the point of the green light approval to the walk through (i.e. when the build is done.) To be clear: this is not an invitation to guess. We don't want your opinion; we want a value that anyone would derive by using the same prediction method and model of your solution. The chief result we look for is prediction in hours. Show us you can research various prediction methods which might be named in our supporting class materials, and then apply them here. This component of your submission should be sufficient to support its use later in the second half of a cost exercise, where you report how much effort the project did take, and then explain why they are the same or different. Presumably you would not want to submit a green light proposal with a cost estimate which predicts there isn't enough time left available to actually do the job, or give a plan which predicts peak burn rates might occur at times when you are heavily conflicted with other course or life obligations.

**Note**: Actual Costs might be the best estimate method for us. We can base our costs based on the small-scale tasks completed in the Scrimmage. Maybe multiply by a seriousness factor (need higher quality product than scrim)

-Parametric relies on knowing many different factors which we may not understand. Used mostly in huge projects such as NASA projects
-Analogous is also a strong option, but we would need to know how TopHat developed and their costs.
```
Each team member needs to fill in the time taken (in hours) to perform each task in their Scrim, this project, or their previous experiences
-front end dev:
-backend dev
	-data schema:
	-database creation:
	-database configuration:
-web design
	-host setup:
	-UI design:
	-advertisement:
-logistics
	-coaching meetings:
	-team meetings:
	-agile stand-ups(weekly?):


# 6. PROJECT ADVERTISEMENT

```
At final delivery we will want a description of the project suitable for showing what 435 students do in the class. Thus, make sure your project proposal reflects a plan for getting the advertisement ready. To be clear: **at proposal time we are not asking for a web site, we are asking for you to show you have planned to get a site ready and also incorporated this activity into your cost estimate**. What we need at the project approval time is a press release that could be used to announce your project. (Yes, this is one of the more important planning tools, as we will discuss in class.) Remember: The advertisement material may not necessarily be the same as what you ship for a final project site, depending on your task. It is a formulation that fits into our server; preparing content on the assumption that you are free to stand up content on your favorite Erlang-on-rails engine might result in you being unpleasantly surprised later

Note: website with Log-in and Sign Up links (student and professor). Website will be mostly advertising what we provide. Actual app will be after log-in
Note: !WE DON'T NEED A SECTION FOR THIS! We can discuss it in the Timeline and Cost Estimate, maybe mentioned in a sentence elsewhere
```



# 7.  IP STATEMENT

 ENGAGE is a pedagogic data collection and administrative task integration software. The University of Maryland grants all the intellectual property rights to students for software developed during their academic experiences. The data collected via de product will satisfy FERPA compliance; the student data collected will be only available to the educator using the product and the individual student. The open-source tools, Javascript, Graphql, and React, used for the development of the product fall under the MIT license.

####UNIVERSITY OF MARYLAND INTELLECTUAL PROPERTY POLICY, section V.D.3
"Students shall own all rights, title, and interests, including Intellectual
Property rights, in Inventions, Software, Research Data and Tangible Research
Materials they create, conceive or reduce to practice in the performance of their
academic and research activities whether or not they use Significant University
Resources provided they are not owned by the University under Section V.D.1."
####MIT License
"A short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code."

The product may be used, modified, and redistributed, any instantiation of the product mention the licenses provided and any modification of the code is explicitly stated. Any other restrictions are subject to the licenses above.